import requests  # Library for making HTTP requests
from bs4 import BeautifulSoup  # Library for parsing HTML content
import re  # Library for regular expressions
import threading  # Library for threading

# Define common API key patterns
patterns = {
    "Google API Key": r"AIza[0-9A-Za-z-_]{35}",  # Pattern for Google API keys
    "AWS Access Key ID": r"AKIA[0-9A-Z]{16}",  # Pattern for AWS Access Key IDs
    "AWS Secret Access Key": r"(?<![A-Z0-9])[A-Za-z0-9/+=]{40}(?![A-Z0-9])",  # Pattern for AWS Secret Access Keys
    "Generic API Key": r"[a-zA-Z0-9]{32,45}",  # Pattern for generic API keys
    "Bearer Token": r"Bearer\s[0-9a-zA-Z\-_]{20,}",  # Pattern for Bearer tokens
}

def fetch_content(url):
    """
    Fetch the content of a URL using requests library
    :param url: URL to fetch
    :return: HTML content of the URL
    """
    response = requests.get(url)
    response.raise_for_status()
    return response.text

def extract_js_files(soup):
    """
    Extract JavaScript file URLs from an HTML soup
    :param soup: BeautifulSoup object
    :return: List of JavaScript file URLs
    """
    scripts = soup.find_all('script', src=True)
    return [script['src'] for script in scripts]

def find_api_keys(content, patterns):
    """
    Find API keys in a given content using regular expressions
    :param content: Content to search for API keys
    :param patterns: Dictionary of API key patterns
    :return: Dictionary of found API keys
    """
    found_keys = {}
    for name, pattern in patterns.items():
        matches = re.findall(pattern, content)
        if matches:
            found_keys[name] = matches
    return found_keys

class JsFileScanner(threading.Thread):
    def __init__(self, js_file, url, patterns, found_keys):
        threading.Thread.__init__(self)
        self.js_file = js_file
        self.url = url
        self.patterns = patterns
        self.found_keys = found_keys

    def run(self):
        try:
            js_content = fetch_content(self.js_file)
            js_keys = find_api_keys(js_content, self.patterns)
            for key, values in js_keys.items():
                if key in self.found_keys:
                    self.found_keys[key].extend(values)
                else:
                    self.found_keys[key] = values
        except requests.RequestException as e:
            print(f"Could not retrieve {self.js_file}: {e}")

def scan_page(url):
    """
    Scan a webpage for API keys
    :param url: URL of the webpage to scan
    :return: Dictionary of found API keys
    """
    print(f"Scanning {url}...")
    page_content = fetch_content(url)
    soup = BeautifulSoup(page_content, 'html.parser')

    # Scan the HTML content
    found_keys = find_api_keys(page_content, patterns)

    # Extract and scan JavaScript files
    js_files = extract_js_files(soup)
    threads = []
    for js_file in js_files:
        if not js_file.startswith('http'):
            js_file = url.rstrip('/') + '/' + js_file.lstrip('/')
        thread = JsFileScanner(js_file, url, patterns, found_keys)
        thread.start()
        threads.append(thread)

    # Wait for all threads to complete
    for thread in threads:
        thread.join()

    return found_keys

def main():
    """
    Main function to scan a webpage for API keys
    """
    url = input("Enter the URL to scan: ")
    found_keys = scan_page(url)

    if found_keys:
        print("Found API keys:")
        for key_type, keys in found_keys.items():
            print(f"{key_type}:")
            for key in keys:
                print(f"  - {key}")
    else:
        print("No API keys found.")

if __name__ == "__main__":
    main()
